# 一、 RADOS 概述

## 1. ceph 三大核心应用

![1704109998223](image/一、Rados概述/1704109998223.png)

- 简而言之，一个 rados 集群由大量 OSD 和少数几个 mon 组成
- osd 是个抽象概念，一般对应一个本地块设备（如一块磁盘或者一个 RAID）
  - 在其工作周期内会占用一些 CPU、内存、网络等物理资源
  - 依赖某个具体的本地对象存储引擎，来访问位于块设备上的数据
  - 数据复制、故障检测和数据恢复都由每个 osd 自动进行，因此即便存储容量上升至 PB 级别甚至以上，系统也不会存在明显的调度和处理瓶颈
- mon 团体（quorum），本质上也是一个集群
  - 是基于高可靠设计用来负责维护和分发集群关键元数据的
  - 也是客户端与 RADOS 集群建立连接的桥梁：客户端会通过咨询 mon 集群来获取关键元数据，然后通过约定的方式（也就是 RBD、RGW、CephFS 等）来访问 rados 集群![1704110321795](image/一、Rados概述/1704110321795.png)
- 集群表：为了去中心化，避免单点故障
  - RADOS 使用一张紧凑的集群表对集群拓扑结构和数据映射规则进行描述
  - 任何时刻，任何持有该表的合法客户端都可以独立地与位于任意位置的 OSD 直接通信
  - 当集群拓扑发生变化时，RADOS 会使这些变化被 MON 捕获，并通过集群表高效传递至所有受影响的 OSD 和客户端，以保证对外提供不中断的业务访问
- CRUSH 算法：一种基于可扩展哈希的受控副本分布算法，用于连接客户端和 OSD，使得客户端可以直接与 OSD 通信


## 2. 存储池与 PG

设计目的：

实现存储资源按需配置


存储池是什么：

存储池：实际上是个虚拟概念，表示一组约束条件
例如可以针对存储池设计一组 CRUSH 规则，限制其只能使用某些相同规格的 OSD，或者尽可能的将所有数据分布分布在物理上隔离的故障域；
也可以针对不同存储用途的存储池指定不同的副本策略，例如若存储池承载的存储应用对时延敏感，就采用多副本策略，反之，如果存储的是一些对时延不敏感的数据，为提升空间则采用 EC 策略更好


存储池之间的策略隔离是如何实现的：

为了实现不同存储池之间的策略隔离，RADOS 就不能将任何应用程序的数据都一步到位的直接写入 OSD 的本地存储设备，由此引入了一个中间结构，称为 PG，通过执行两次映射来保证策略隔离的同时数据落盘

![1704294346575](image/一、Rados概述/1704294346575.png)
